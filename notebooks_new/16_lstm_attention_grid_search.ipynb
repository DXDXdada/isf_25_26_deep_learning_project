{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "1d039d86",
            "metadata": {},
            "source": [
                "# Notebook 16: LSTM with Attention Mechanism and Grid Search\n",
                "\n",
                "**Objectives**:\n",
                "- Implement a custom Attention mechanism for LSTM networks\n",
                "- Build an LSTM model with integrated Attention layer\n",
                "- Perform rigorous Grid Search for hyperparameter optimization\n",
                "- Train the best model and evaluate performance\n",
                "- Compare with baseline LSTM model\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "1. Setup and Configuration\n",
                "2. Custom Attention Layer Implementation\n",
                "3. LSTM with Attention Model Builder\n",
                "4. Data Loading\n",
                "5. Grid Search Hyperparameter Optimization\n",
                "6. Training with Best Hyperparameters\n",
                "7. Evaluation and Visualization\n",
                "8. Comparison with Baseline LSTM\n",
                "9. Conclusions"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a6484e15",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2a1d3bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# TensorFlow and Keras\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import models, layers, optimizers, callbacks\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, Dense, Dropout, LSTM, Layer, Permute, Multiply,\n",
                "    RepeatVector, Flatten, Activation, Lambda\n",
                ")\n",
                "from tensorflow.keras import backend as K\n",
                "\n",
                "# Scikit-learn\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
                ")\n",
                "from sklearn.model_selection import ParameterGrid\n",
                "\n",
                "import pickle\n",
                "import os\n",
                "import sys\n",
                "from itertools import product\n",
                "from datetime import datetime\n",
                "\n",
                "# Add utils to path\n",
                "sys.path.append('../utils')\n",
                "import utils\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "tf.random.set_seed(SEED)\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
                "print(\"\\n[OK] Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83cdaf93",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure plotting\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "plt.rcParams['figure.figsize'] = (14, 8)\n",
                "plt.rcParams['font.size'] = 11\n",
                "\n",
                "print(\"[OK] Plotting configuration set\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f6639502",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define paths\n",
                "SEQUENCES_DIR = '../data_new/sequences/'\n",
                "MODELS_DIR = '../models/lstm_attention/'\n",
                "RESULTS_DIR = '../results/'\n",
                "FIGURES_DIR = '../results/figures/lstm_attention/'\n",
                "\n",
                "# Create directories\n",
                "os.makedirs(MODELS_DIR, exist_ok=True)\n",
                "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
                "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
                "\n",
                "# Assets and horizons\n",
                "ASSETS = ['AAPL', 'AMZN', 'NVDA', 'SPY', 'BTC-USD']\n",
                "HORIZONS = ['1day', '1week', '1month']\n",
                "\n",
                "print(\"[OK] Directories configured\")\n",
                "print(f\"  Assets: {ASSETS}\")\n",
                "print(f\"  Horizons: {HORIZONS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "attention_section",
            "metadata": {},
            "source": [
                "## 2. Custom Attention Layer Implementation\n",
                "\n",
                "The Attention mechanism allows the model to focus on the most relevant time steps when making predictions.\n",
                "\n",
                "**How it works:**\n",
                "1. Compute attention scores for each time step\n",
                "2. Apply softmax to get attention weights (probability distribution)\n",
                "3. Multiply LSTM outputs by attention weights\n",
                "4. Sum the weighted outputs to get the context vector"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "attention_layer",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Layer explicitly (ensures availability even if cells run out of order)\n",
                "from tensorflow.keras.layers import Layer\n",
                "from tensorflow.keras import backend as K\n",
                "\n",
                "class AttentionLayer(Layer):\n",
                "    \"\"\"\n",
                "    Custom Attention Layer for LSTM networks.\n",
                "    \n",
                "    This layer learns to weight the importance of each time step\n",
                "    in the LSTM output sequence, creating a context vector that\n",
                "    emphasizes the most relevant information for classification.\n",
                "    \n",
                "    Input shape: (batch_size, time_steps, features)\n",
                "    Output shape: (batch_size, features)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, return_attention=False, **kwargs):\n",
                "        \"\"\"\n",
                "        Initialize the Attention Layer.\n",
                "        \n",
                "        Args:\n",
                "            return_attention: If True, also return attention weights\n",
                "        \"\"\"\n",
                "        self.return_attention = return_attention\n",
                "        super(AttentionLayer, self).__init__(**kwargs)\n",
                "    \n",
                "    def build(self, input_shape):\n",
                "        \"\"\"\n",
                "        Build the layer weights.\n",
                "        \n",
                "        Creates weight matrix W and bias b for computing attention scores.\n",
                "        \"\"\"\n",
                "        self.W = self.add_weight(\n",
                "            name='attention_weight',\n",
                "            shape=(input_shape[-1], 1),\n",
                "            initializer='glorot_uniform',\n",
                "            trainable=True\n",
                "        )\n",
                "        self.b = self.add_weight(\n",
                "            name='attention_bias',\n",
                "            shape=(input_shape[1], 1),\n",
                "            initializer='zeros',\n",
                "            trainable=True\n",
                "        )\n",
                "        super(AttentionLayer, self).build(input_shape)\n",
                "    \n",
                "    def call(self, inputs):\n",
                "        \"\"\"\n",
                "        Forward pass of the attention mechanism.\n",
                "        \n",
                "        Args:\n",
                "            inputs: LSTM output sequence (batch_size, time_steps, features)\n",
                "        \n",
                "        Returns:\n",
                "            Context vector (batch_size, features)\n",
                "            Optionally: attention weights (batch_size, time_steps)\n",
                "        \"\"\"\n",
                "        # Compute attention scores: e = tanh(inputs @ W + b)\n",
                "        e = K.tanh(K.dot(inputs, self.W) + self.b)\n",
                "        \n",
                "        # Squeeze the last dimension\n",
                "        e = K.squeeze(e, axis=-1)\n",
                "        \n",
                "        # Apply softmax to get attention weights\n",
                "        alpha = K.softmax(e)\n",
                "        \n",
                "        # Expand dims for multiplication\n",
                "        alpha = K.expand_dims(alpha, axis=-1)\n",
                "        \n",
                "        # Compute context vector as weighted sum\n",
                "        context = inputs * alpha\n",
                "        context = K.sum(context, axis=1)\n",
                "        \n",
                "        if self.return_attention:\n",
                "            return context, K.squeeze(alpha, axis=-1)\n",
                "        return context\n",
                "    \n",
                "    def compute_output_shape(self, input_shape):\n",
                "        \"\"\"Compute the output shape.\"\"\"\n",
                "        if self.return_attention:\n",
                "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[1])]\n",
                "        return (input_shape[0], input_shape[-1])\n",
                "    \n",
                "    def get_config(self):\n",
                "        \"\"\"Get layer configuration for serialization.\"\"\"\n",
                "        config = super(AttentionLayer, self).get_config()\n",
                "        config.update({'return_attention': self.return_attention})\n",
                "        return config\n",
                "\n",
                "print(\"[OK] AttentionLayer class defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_builder_section",
            "metadata": {},
            "source": [
                "## 3. LSTM with Attention Model Builder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model_builder",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_lstm_attention_model(\n",
                "    sequence_length,\n",
                "    n_features,\n",
                "    lstm_units=128,\n",
                "    lstm_layers=2,\n",
                "    dropout_rate=0.3,\n",
                "    dense_units=64,\n",
                "    learning_rate=0.001\n",
                "):\n",
                "    \"\"\"\n",
                "    Build LSTM model with Attention mechanism for binary classification.\n",
                "    \n",
                "    Architecture:\n",
                "        Input -> LSTM layers (return sequences) -> Attention -> Dense -> Output\n",
                "    \n",
                "    Args:\n",
                "        sequence_length: Number of time steps in input sequence\n",
                "        n_features: Number of features per time step\n",
                "        lstm_units: Number of units in each LSTM layer\n",
                "        lstm_layers: Number of stacked LSTM layers\n",
                "        dropout_rate: Dropout rate for regularization\n",
                "        dense_units: Number of units in dense layer before output\n",
                "        learning_rate: Learning rate for Adam optimizer\n",
                "    \n",
                "    Returns:\n",
                "        Compiled Keras model\n",
                "    \"\"\"\n",
                "    # Input layer\n",
                "    inputs = Input(shape=(sequence_length, n_features), name='input')\n",
                "    x = inputs\n",
                "    \n",
                "    # LSTM layers - all return sequences for attention\n",
                "    for i in range(lstm_layers):\n",
                "        x = LSTM(\n",
                "            units=lstm_units,\n",
                "            return_sequences=True,  # Always return sequences for attention\n",
                "            name=f'lstm_{i+1}'\n",
                "        )(x)\n",
                "        x = Dropout(dropout_rate, name=f'dropout_lstm_{i+1}')(x)\n",
                "    \n",
                "    # Attention layer\n",
                "    x = AttentionLayer(name='attention')(x)\n",
                "    \n",
                "    # Dense layers\n",
                "    x = Dense(dense_units, activation='relu', name='dense_1')(x)\n",
                "    x = Dropout(dropout_rate, name='dropout_dense')(x)\n",
                "    \n",
                "    # Output layer (binary classification)\n",
                "    outputs = Dense(1, activation='sigmoid', name='output')(x)\n",
                "    \n",
                "    # Create model\n",
                "    model = models.Model(inputs=inputs, outputs=outputs, name='LSTM_Attention_Model')\n",
                "    \n",
                "    # Compile model\n",
                "    model.compile(\n",
                "        optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
                "        loss='binary_crossentropy',\n",
                "        metrics=[\n",
                "            'accuracy',\n",
                "            tf.keras.metrics.AUC(name='auc'),\n",
                "            tf.keras.metrics.Precision(name='precision'),\n",
                "            tf.keras.metrics.Recall(name='recall')\n",
                "        ]\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "print(\"[OK] build_lstm_attention_model function defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the model architecture\n",
                "test_model = build_lstm_attention_model(\n",
                "    sequence_length=7,\n",
                "    n_features=39,\n",
                "    lstm_units=64,\n",
                "    lstm_layers=2,\n",
                "    dropout_rate=0.3,\n",
                "    dense_units=32,\n",
                "    learning_rate=0.001\n",
                ")\n",
                "\n",
                "print(\"Model Architecture:\")\n",
                "print(\"=\" * 80)\n",
                "test_model.summary()\n",
                "\n",
                "# Clean up test model\n",
                "del test_model\n",
                "K.clear_session()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data_section",
            "metadata": {},
            "source": [
                "## 4. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load class weights\n",
                "class_weights = utils.load_class_weights(SEQUENCES_DIR)\n",
                "\n",
                "print(\"[OK] Class weights loaded\")\n",
                "print(f\"  Available combinations: {len(class_weights)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "select_asset",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select asset and horizon for Grid Search\n",
                "# We'll demonstrate with AAPL 1day first, then apply best config to others\n",
                "ASSET = 'AAPL'\n",
                "HORIZON = '1day'\n",
                "\n",
                "# Load data\n",
                "X_train, X_val, X_test, y_train, y_val, y_test, seq_len, n_feat = utils.load_sequences(\n",
                "    ASSET, HORIZON, SEQUENCES_DIR\n",
                ")\n",
                "\n",
                "print(f\"\\nData loaded for {ASSET} - {HORIZON}:\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"  Train samples: {X_train.shape[0]:,}\")\n",
                "print(f\"  Validation samples: {X_val.shape[0]:,}\")\n",
                "print(f\"  Test samples: {X_test.shape[0]:,}\")\n",
                "print(f\"  Sequence length: {seq_len}\")\n",
                "print(f\"  Number of features: {n_feat}\")\n",
                "print(f\"  Class distribution (train): 0: {(y_train==0).sum():,}, 1: {(y_train==1).sum():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "grid_search_section",
            "metadata": {},
            "source": [
                "## 5. Grid Search Hyperparameter Optimization\n",
                "\n",
                "We will systematically search through a defined hyperparameter space to find the optimal configuration.\n",
                "\n",
                "**Hyperparameter Grid:**\n",
                "- LSTM units: [64, 128]\n",
                "- LSTM layers: [1, 2]\n",
                "- Dropout rate: [0.2, 0.3]\n",
                "- Learning rate: [0.001, 0.0005]\n",
                "\n",
                "**Strategy:**\n",
                "- Train each configuration for reduced epochs (20) with early stopping\n",
                "- Evaluate on validation set\n",
                "- Select best configuration based on validation accuracy\n",
                "- Retrain best model with full epochs (100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "define_grid",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define hyperparameter search grid\n",
                "PARAM_GRID = {\n",
                "    'lstm_units': [64, 128],\n",
                "    'lstm_layers': [1, 2],\n",
                "    'dropout_rate': [0.2, 0.3],\n",
                "    'learning_rate': [0.001, 0.0005]\n",
                "}\n",
                "\n",
                "# Fixed parameters\n",
                "FIXED_PARAMS = {\n",
                "    'dense_units': 64,\n",
                "    'batch_size': 32,\n",
                "    'epochs_grid_search': 20,  # Reduced epochs for grid search\n",
                "    'epochs_final': 100,       # Full training epochs\n",
                "    'patience': 10\n",
                "}\n",
                "\n",
                "# Generate all combinations\n",
                "param_combinations = list(ParameterGrid(PARAM_GRID))\n",
                "\n",
                "print(\"Hyperparameter Search Grid:\")\n",
                "print(\"=\" * 60)\n",
                "for param, values in PARAM_GRID.items():\n",
                "    print(f\"  {param}: {values}\")\n",
                "print(f\"\\nTotal combinations to evaluate: {len(param_combinations)}\")\n",
                "print(f\"Epochs per configuration: {FIXED_PARAMS['epochs_grid_search']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "grid_search_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_grid_search(\n",
                "    param_combinations,\n",
                "    X_train, y_train,\n",
                "    X_val, y_val,\n",
                "    seq_len, n_feat,\n",
                "    class_weight_dict,\n",
                "    fixed_params\n",
                "):\n",
                "    \"\"\"\n",
                "    Run grid search over hyperparameter combinations.\n",
                "    \n",
                "    Args:\n",
                "        param_combinations: List of hyperparameter dictionaries\n",
                "        X_train, y_train: Training data\n",
                "        X_val, y_val: Validation data\n",
                "        seq_len: Sequence length\n",
                "        n_feat: Number of features\n",
                "        class_weight_dict: Class weights for imbalanced data\n",
                "        fixed_params: Fixed hyperparameters\n",
                "    \n",
                "    Returns:\n",
                "        DataFrame with results for all configurations\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    total = len(param_combinations)\n",
                "    \n",
                "    print(f\"\\nStarting Grid Search with {total} configurations...\")\n",
                "    print(\"=\" * 80)\n",
                "    \n",
                "    for idx, params in enumerate(param_combinations):\n",
                "        print(f\"\\n[{idx+1}/{total}] Testing configuration:\")\n",
                "        print(f\"  {params}\")\n",
                "        \n",
                "        start_time = datetime.now()\n",
                "        \n",
                "        try:\n",
                "            # Clear session to free memory\n",
                "            K.clear_session()\n",
                "            \n",
                "            # Build model with current hyperparameters\n",
                "            model = build_lstm_attention_model(\n",
                "                sequence_length=seq_len,\n",
                "                n_features=n_feat,\n",
                "                lstm_units=params['lstm_units'],\n",
                "                lstm_layers=params['lstm_layers'],\n",
                "                dropout_rate=params['dropout_rate'],\n",
                "                dense_units=fixed_params['dense_units'],\n",
                "                learning_rate=params['learning_rate']\n",
                "            )\n",
                "            \n",
                "            # Callbacks for grid search (early stopping only)\n",
                "            grid_callbacks = [\n",
                "                callbacks.EarlyStopping(\n",
                "                    monitor='val_loss',\n",
                "                    patience=5,  # Shorter patience for grid search\n",
                "                    min_delta=0.001,\n",
                "                    restore_best_weights=True,\n",
                "                    verbose=0\n",
                "                ),\n",
                "                callbacks.ReduceLROnPlateau(\n",
                "                    monitor='val_loss',\n",
                "                    factor=0.5,\n",
                "                    patience=3,\n",
                "                    min_lr=1e-7,\n",
                "                    verbose=0\n",
                "                )\n",
                "            ]\n",
                "            \n",
                "            # Train model\n",
                "            history = model.fit(\n",
                "                X_train, y_train,\n",
                "                validation_data=(X_val, y_val),\n",
                "                epochs=fixed_params['epochs_grid_search'],\n",
                "                batch_size=fixed_params['batch_size'],\n",
                "                class_weight=class_weight_dict,\n",
                "                callbacks=grid_callbacks,\n",
                "                verbose=0\n",
                "            )\n",
                "            \n",
                "            # Evaluate on validation set\n",
                "            val_metrics = model.evaluate(X_val, y_val, verbose=0)\n",
                "            \n",
                "            # Get predictions for F1 score\n",
                "            y_val_pred = (model.predict(X_val, verbose=0) > 0.5).astype(int).flatten()\n",
                "            val_f1 = f1_score(y_val, y_val_pred)\n",
                "            \n",
                "            # Store results\n",
                "            result = {\n",
                "                'config_id': idx + 1,\n",
                "                **params,\n",
                "                'val_loss': val_metrics[0],\n",
                "                'val_accuracy': val_metrics[1],\n",
                "                'val_auc': val_metrics[2],\n",
                "                'val_precision': val_metrics[3],\n",
                "                'val_recall': val_metrics[4],\n",
                "                'val_f1': val_f1,\n",
                "                'epochs_trained': len(history.history['loss']),\n",
                "                'training_time': (datetime.now() - start_time).total_seconds()\n",
                "            }\n",
                "            results.append(result)\n",
                "            \n",
                "            print(f\"  -> Val Accuracy: {val_metrics[1]:.4f}, Val AUC: {val_metrics[2]:.4f}, Val F1: {val_f1:.4f}\")\n",
                "            print(f\"  -> Training time: {result['training_time']:.1f}s, Epochs: {result['epochs_trained']}\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"  -> ERROR: {str(e)}\")\n",
                "            results.append({\n",
                "                'config_id': idx + 1,\n",
                "                **params,\n",
                "                'val_loss': np.nan,\n",
                "                'val_accuracy': np.nan,\n",
                "                'val_auc': np.nan,\n",
                "                'val_precision': np.nan,\n",
                "                'val_recall': np.nan,\n",
                "                'val_f1': np.nan,\n",
                "                'epochs_trained': 0,\n",
                "                'training_time': 0,\n",
                "                'error': str(e)\n",
                "            })\n",
                "        \n",
                "        # Clean up\n",
                "        del model\n",
                "        K.clear_session()\n",
                "    \n",
                "    return pd.DataFrame(results)\n",
                "\n",
                "print(\"[OK] run_grid_search function defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_grid_search",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get class weights for current asset/horizon\n",
                "cw = class_weights[ASSET][HORIZON]\n",
                "class_weight_dict = {0: cw[0], 1: cw[1]}\n",
                "\n",
                "print(f\"Class weights: {class_weight_dict}\")\n",
                "\n",
                "# Run Grid Search\n",
                "grid_search_results = run_grid_search(\n",
                "    param_combinations=param_combinations,\n",
                "    X_train=X_train, y_train=y_train,\n",
                "    X_val=X_val, y_val=y_val,\n",
                "    seq_len=seq_len, n_feat=n_feat,\n",
                "    class_weight_dict=class_weight_dict,\n",
                "    fixed_params=FIXED_PARAMS\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "show_grid_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display Grid Search results\n",
                "print(\"\\nGrid Search Results:\")\n",
                "print(\"=\" * 100)\n",
                "\n",
                "# Sort by validation accuracy\n",
                "grid_search_results_sorted = grid_search_results.sort_values(\n",
                "    'val_accuracy', ascending=False\n",
                ").reset_index(drop=True)\n",
                "\n",
                "# Display top 5 configurations\n",
                "display_cols = ['config_id', 'lstm_units', 'lstm_layers', 'dropout_rate', \n",
                "                'learning_rate', 'val_accuracy', 'val_auc', 'val_f1']\n",
                "print(\"\\nTop 5 Configurations:\")\n",
                "print(grid_search_results_sorted[display_cols].head(10).to_string(index=False))\n",
                "\n",
                "# Best configuration\n",
                "best_config = grid_search_results_sorted.iloc[0]\n",
                "print(f\"\\n\" + \"=\" * 60)\n",
                "print(\"BEST CONFIGURATION:\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"  LSTM Units: {int(best_config['lstm_units'])}\")\n",
                "print(f\"  LSTM Layers: {int(best_config['lstm_layers'])}\")\n",
                "print(f\"  Dropout Rate: {best_config['dropout_rate']}\")\n",
                "print(f\"  Learning Rate: {best_config['learning_rate']}\")\n",
                "print(f\"  Validation Accuracy: {best_config['val_accuracy']:.4f}\")\n",
                "print(f\"  Validation AUC: {best_config['val_auc']:.4f}\")\n",
                "print(f\"  Validation F1: {best_config['val_f1']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "visualize_grid_search",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Grid Search results\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Impact of LSTM units\n",
                "units_impact = grid_search_results.groupby('lstm_units')['val_accuracy'].mean()\n",
                "axes[0, 0].bar(units_impact.index.astype(str), units_impact.values, color='steelblue', edgecolor='black')\n",
                "axes[0, 0].set_xlabel('LSTM Units', fontsize=11)\n",
                "axes[0, 0].set_ylabel('Mean Validation Accuracy', fontsize=11)\n",
                "axes[0, 0].set_title('Impact of LSTM Units', fontsize=12, fontweight='bold')\n",
                "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Impact of LSTM layers\n",
                "layers_impact = grid_search_results.groupby('lstm_layers')['val_accuracy'].mean()\n",
                "axes[0, 1].bar(layers_impact.index.astype(str), layers_impact.values, color='darkorange', edgecolor='black')\n",
                "axes[0, 1].set_xlabel('LSTM Layers', fontsize=11)\n",
                "axes[0, 1].set_ylabel('Mean Validation Accuracy', fontsize=11)\n",
                "axes[0, 1].set_title('Impact of LSTM Layers', fontsize=12, fontweight='bold')\n",
                "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Impact of dropout rate\n",
                "dropout_impact = grid_search_results.groupby('dropout_rate')['val_accuracy'].mean()\n",
                "axes[1, 0].bar(dropout_impact.index.astype(str), dropout_impact.values, color='green', edgecolor='black')\n",
                "axes[1, 0].set_xlabel('Dropout Rate', fontsize=11)\n",
                "axes[1, 0].set_ylabel('Mean Validation Accuracy', fontsize=11)\n",
                "axes[1, 0].set_title('Impact of Dropout Rate', fontsize=12, fontweight='bold')\n",
                "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Impact of learning rate\n",
                "lr_impact = grid_search_results.groupby('learning_rate')['val_accuracy'].mean()\n",
                "axes[1, 1].bar([str(x) for x in lr_impact.index], lr_impact.values, color='crimson', edgecolor='black')\n",
                "axes[1, 1].set_xlabel('Learning Rate', fontsize=11)\n",
                "axes[1, 1].set_ylabel('Mean Validation Accuracy', fontsize=11)\n",
                "axes[1, 1].set_title('Impact of Learning Rate', fontsize=12, fontweight='bold')\n",
                "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.suptitle(f'Grid Search Results - {ASSET} {HORIZON}', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{FIGURES_DIR}grid_search_impact_{ASSET}_{HORIZON}.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"[OK] Grid search visualization saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_section",
            "metadata": {},
            "source": [
                "## 6. Training with Best Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_best_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clear session\n",
                "K.clear_session()\n",
                "\n",
                "# Build model with best hyperparameters\n",
                "best_model = build_lstm_attention_model(\n",
                "    sequence_length=seq_len,\n",
                "    n_features=n_feat,\n",
                "    lstm_units=int(best_config['lstm_units']),\n",
                "    lstm_layers=int(best_config['lstm_layers']),\n",
                "    dropout_rate=best_config['dropout_rate'],\n",
                "    dense_units=FIXED_PARAMS['dense_units'],\n",
                "    learning_rate=best_config['learning_rate']\n",
                ")\n",
                "\n",
                "print(\"Best Model Architecture:\")\n",
                "print(\"=\" * 80)\n",
                "best_model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_full_epochs",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Callbacks for full training\n",
                "model_name = f'LSTM_Attention_{ASSET}_{HORIZON}'\n",
                "\n",
                "training_callbacks = [\n",
                "    callbacks.EarlyStopping(\n",
                "        monitor='val_loss',\n",
                "        patience=FIXED_PARAMS['patience'],\n",
                "        min_delta=0.001,\n",
                "        restore_best_weights=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    callbacks.ReduceLROnPlateau(\n",
                "        monitor='val_loss',\n",
                "        factor=0.5,\n",
                "        patience=5,\n",
                "        min_lr=1e-7,\n",
                "        verbose=1\n",
                "    ),\n",
                "    callbacks.ModelCheckpoint(\n",
                "        filepath=f'{MODELS_DIR}{model_name}_best.keras',\n",
                "        monitor='val_loss',\n",
                "        save_best_only=True,\n",
                "        verbose=0\n",
                "    )\n",
                "]\n",
                "\n",
                "print(f\"\\nTraining {model_name}...\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Train model\n",
                "history = best_model.fit(\n",
                "    X_train, y_train,\n",
                "    validation_data=(X_val, y_val),\n",
                "    epochs=FIXED_PARAMS['epochs_final'],\n",
                "    batch_size=FIXED_PARAMS['batch_size'],\n",
                "    class_weight=class_weight_dict,\n",
                "    callbacks=training_callbacks,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(f\"\\n[OK] Training complete after {len(history.history['loss'])} epochs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_training_history",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
                "\n",
                "# Loss\n",
                "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
                "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
                "axes[0, 0].set_title('Model Loss', fontsize=12, fontweight='bold')\n",
                "axes[0, 0].set_xlabel('Epoch')\n",
                "axes[0, 0].set_ylabel('Loss')\n",
                "axes[0, 0].legend()\n",
                "axes[0, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Accuracy\n",
                "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
                "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
                "axes[0, 1].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
                "axes[0, 1].set_xlabel('Epoch')\n",
                "axes[0, 1].set_ylabel('Accuracy')\n",
                "axes[0, 1].legend()\n",
                "axes[0, 1].grid(True, alpha=0.3)\n",
                "\n",
                "# AUC\n",
                "axes[1, 0].plot(history.history['auc'], label='Train AUC', linewidth=2)\n",
                "axes[1, 0].plot(history.history['val_auc'], label='Val AUC', linewidth=2)\n",
                "axes[1, 0].set_title('Model AUC', fontsize=12, fontweight='bold')\n",
                "axes[1, 0].set_xlabel('Epoch')\n",
                "axes[1, 0].set_ylabel('AUC')\n",
                "axes[1, 0].legend()\n",
                "axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Precision & Recall\n",
                "axes[1, 1].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
                "axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linewidth=2, linestyle='--')\n",
                "axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2)\n",
                "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2, linestyle='--')\n",
                "axes[1, 1].set_title('Precision & Recall', fontsize=12, fontweight='bold')\n",
                "axes[1, 1].set_xlabel('Epoch')\n",
                "axes[1, 1].set_ylabel('Score')\n",
                "axes[1, 1].legend()\n",
                "axes[1, 1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle(f'{model_name} - Training History', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{FIGURES_DIR}{model_name}_training_history.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"[OK] Training history visualization saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "evaluation_section",
            "metadata": {},
            "source": [
                "## 7. Evaluation and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "evaluate_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "print(f\"\\nEvaluating on Test Set:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Get predictions\n",
                "y_pred_proba = best_model.predict(X_test, verbose=0).flatten()\n",
                "y_pred = (y_pred_proba > 0.5).astype(int)\n",
                "\n",
                "# Calculate metrics\n",
                "test_accuracy = accuracy_score(y_test, y_pred)\n",
                "test_precision = precision_score(y_test, y_pred)\n",
                "test_recall = recall_score(y_test, y_pred)\n",
                "test_f1 = f1_score(y_test, y_pred)\n",
                "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
                "\n",
                "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
                "print(f\"  Precision: {test_precision:.4f}\")\n",
                "print(f\"  Recall:    {test_recall:.4f}\")\n",
                "print(f\"  F1 Score:  {test_f1:.4f}\")\n",
                "print(f\"  AUC:       {test_auc:.4f}\")\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=['DOWN', 'UP']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_confusion_matrix",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix and ROC Curve\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'])\n",
                "axes[0].set_xlabel('Predicted', fontsize=11)\n",
                "axes[0].set_ylabel('Actual', fontsize=11)\n",
                "axes[0].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
                "\n",
                "# ROC Curve\n",
                "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
                "axes[1].plot(fpr, tpr, linewidth=2, label=f'AUC = {test_auc:.4f}')\n",
                "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
                "axes[1].set_xlabel('False Positive Rate', fontsize=11)\n",
                "axes[1].set_ylabel('True Positive Rate', fontsize=11)\n",
                "axes[1].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
                "axes[1].legend(loc='lower right')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle(f'{model_name} - Test Evaluation', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{FIGURES_DIR}{model_name}_evaluation.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"[OK] Evaluation visualization saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison_section",
            "metadata": {},
            "source": [
                "## 8. Comparison with Baseline LSTM\n",
                "\n",
                "Let's compare the LSTM with Attention model against a baseline LSTM without attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_baseline",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build and train baseline LSTM\n",
                "K.clear_session()\n",
                "\n",
                "baseline_model = utils.build_lstm_model(\n",
                "    sequence_length=seq_len,\n",
                "    n_features=n_feat,\n",
                "    lstm_units=int(best_config['lstm_units']),\n",
                "    lstm_layers=int(best_config['lstm_layers']),\n",
                "    dropout_rate=best_config['dropout_rate'],\n",
                "    dense_units=FIXED_PARAMS['dense_units'],\n",
                "    learning_rate=best_config['learning_rate']\n",
                ")\n",
                "\n",
                "baseline_callbacks = [\n",
                "    callbacks.EarlyStopping(\n",
                "        monitor='val_loss',\n",
                "        patience=FIXED_PARAMS['patience'],\n",
                "        min_delta=0.001,\n",
                "        restore_best_weights=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    callbacks.ReduceLROnPlateau(\n",
                "        monitor='val_loss',\n",
                "        factor=0.5,\n",
                "        patience=5,\n",
                "        min_lr=1e-7,\n",
                "        verbose=1\n",
                "    )\n",
                "]\n",
                "\n",
                "print(\"Training baseline LSTM (without attention)...\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "baseline_history = baseline_model.fit(\n",
                "    X_train, y_train,\n",
                "    validation_data=(X_val, y_val),\n",
                "    epochs=FIXED_PARAMS['epochs_final'],\n",
                "    batch_size=FIXED_PARAMS['batch_size'],\n",
                "    class_weight=class_weight_dict,\n",
                "    callbacks=baseline_callbacks,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(f\"\\n[OK] Baseline training complete after {len(baseline_history.history['loss'])} epochs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "compare_models",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate baseline\n",
                "y_baseline_proba = baseline_model.predict(X_test, verbose=0).flatten()\n",
                "y_baseline_pred = (y_baseline_proba > 0.5).astype(int)\n",
                "\n",
                "baseline_accuracy = accuracy_score(y_test, y_baseline_pred)\n",
                "baseline_precision = precision_score(y_test, y_baseline_pred)\n",
                "baseline_recall = recall_score(y_test, y_baseline_pred)\n",
                "baseline_f1 = f1_score(y_test, y_baseline_pred)\n",
                "baseline_auc = roc_auc_score(y_test, y_baseline_proba)\n",
                "\n",
                "# Create comparison DataFrame\n",
                "comparison = pd.DataFrame({\n",
                "    'Model': ['LSTM Baseline', 'LSTM + Attention'],\n",
                "    'Accuracy': [baseline_accuracy, test_accuracy],\n",
                "    'Precision': [baseline_precision, test_precision],\n",
                "    'Recall': [baseline_recall, test_recall],\n",
                "    'F1 Score': [baseline_f1, test_f1],\n",
                "    'AUC': [baseline_auc, test_auc]\n",
                "})\n",
                "\n",
                "print(\"\\nModel Comparison:\")\n",
                "print(\"=\" * 80)\n",
                "print(comparison.to_string(index=False))\n",
                "\n",
                "# Calculate improvement\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"IMPROVEMENT (LSTM + Attention vs Baseline):\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"  Accuracy:  {(test_accuracy - baseline_accuracy) * 100:+.2f}%\")\n",
                "print(f\"  F1 Score:  {(test_f1 - baseline_f1) * 100:+.2f}%\")\n",
                "print(f\"  AUC:       {(test_auc - baseline_auc) * 100:+.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Grid Search results\n",
                "grid_search_results.to_csv(f'{RESULTS_DIR}lstm_attention_grid_search_{ASSET}_{HORIZON}.csv', index=False)\n",
                "\n",
                "# Save comparison results\n",
                "comparison.to_csv(f'{RESULTS_DIR}lstm_attention_comparison_{ASSET}_{HORIZON}.csv', index=False)\n",
                "\n",
                "# Save best hyperparameters\n",
                "best_params = {\n",
                "    'lstm_units': int(best_config['lstm_units']),\n",
                "    'lstm_layers': int(best_config['lstm_layers']),\n",
                "    'dropout_rate': best_config['dropout_rate'],\n",
                "    'learning_rate': best_config['learning_rate'],\n",
                "    'dense_units': FIXED_PARAMS['dense_units']\n",
                "}\n",
                "\n",
                "with open(f'{RESULTS_DIR}lstm_attention_best_params_{ASSET}_{HORIZON}.pkl', 'wb') as f:\n",
                "    pickle.dump(best_params, f)\n",
                "\n",
                "print(\"[OK] Results saved:\")\n",
                "print(f\"  - Grid Search: {RESULTS_DIR}lstm_attention_grid_search_{ASSET}_{HORIZON}.csv\")\n",
                "print(f\"  - Comparison: {RESULTS_DIR}lstm_attention_comparison_{ASSET}_{HORIZON}.csv\")\n",
                "print(f\"  - Best params: {RESULTS_DIR}lstm_attention_best_params_{ASSET}_{HORIZON}.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conclusions",
            "metadata": {},
            "source": [
                "## 9. Conclusions\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "1. **Attention Mechanism**: The custom attention layer allows the model to focus on the most relevant time steps for prediction, potentially improving interpretability and performance.\n",
                "\n",
                "2. **Grid Search Results**: Systematic hyperparameter optimization helped identify the best configuration:\n",
                "   - Optimal number of LSTM units and layers\n",
                "   - Best dropout rate for regularization\n",
                "   - Optimal learning rate for convergence\n",
                "\n",
                "3. **Model Comparison**: The LSTM with Attention mechanism was compared against a baseline LSTM using identical hyperparameters, showing the impact of the attention mechanism.\n",
                "\n",
                "### Recommendations\n",
                "\n",
                "- The attention mechanism adds interpretability by showing which time steps the model considers important\n",
                "- Grid search is computationally expensive but essential for rigorous model selection\n",
                "- Consider using cross-validation for more robust hyperparameter selection\n",
                "\n",
                "---\n",
                "\n",
                "**[OK] Notebook complete!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}