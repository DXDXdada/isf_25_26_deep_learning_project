{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ece7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, optimizers, callbacks\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Input, GlobalAveragePooling1D,\n",
    "    MultiHeadAttention, LayerNormalization\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pickle, os\n",
    "from datetime import datetime\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "SEQUENCES_DIR = '../data_new/sequences/'\n",
    "MODELS_DIR = '../models/transformer/'\n",
    "RESULTS_DIR = '../results/'\n",
    "FIGURES_DIR = '../results/figures/transformer/'\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "ASSETS = ['AAPL', 'AMZN', 'NVDA', 'SPY', 'BTC-USD']\n",
    "HORIZONS = ['1day', '1week', '1month']\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(\"[OK] Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Block\n",
    "def transformer_encoder_block(inputs, head_size, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    x1 = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff_output = Dense(ff_dim, activation='relu')(x1)\n",
    "    ff_output = Dropout(dropout_rate)(ff_output)\n",
    "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "    ff_output = Dropout(dropout_rate)(ff_output)\n",
    "    x2 = LayerNormalization(epsilon=1e-6)(x1 + ff_output)\n",
    "    \n",
    "    return x2\n",
    "\n",
    "def build_transformer_model(\n",
    "    sequence_length, n_features,\n",
    "    num_transformer_blocks=2,\n",
    "    head_size=256, num_heads=4,\n",
    "    ff_dim=256, dropout_rate=0.2,\n",
    "    dense_units=128, learning_rate=0.001\n",
    "):\n",
    "    inputs = Input(shape=(sequence_length, n_features))\n",
    "    x = inputs\n",
    "    \n",
    "    # Stack transformer encoder blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder_block(x, head_size, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='Transformer_Model')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_sequences(asset, horizon):\n",
    "    filepath = f'{SEQUENCES_DIR}{asset}_{horizon}_sequences.npz'\n",
    "    data = np.load(filepath)\n",
    "    return (data['X_train'], data['X_val'], data['X_test'],\n",
    "            data['y_train'], data['y_val'], data['y_test'],\n",
    "            int(data['sequence_length']), int(data['n_features']))\n",
    "\n",
    "def load_class_weights():\n",
    "    with open(f'{SEQUENCES_DIR}class_weights.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_callbacks(model_name, patience=10):\n",
    "    return [\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, verbose=1),\n",
    "        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),\n",
    "        callbacks.ModelCheckpoint(filepath=f'{MODELS_DIR}{model_name}_best.h5', monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "class_weights = load_class_weights()\n",
    "print(\"[OK] Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete training loop\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting Transformer training...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for asset in ASSETS:\n",
    "    for horizon in HORIZONS:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training: {asset} - {horizon}\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, seq_len, n_feat = load_sequences(asset, horizon)\n",
    "        \n",
    "        model = build_transformer_model(\n",
    "            sequence_length=seq_len, n_features=n_feat,\n",
    "            num_transformer_blocks=2, num_heads=4,\n",
    "            head_size=256, ff_dim=256, dropout_rate=0.2\n",
    "        )\n",
    "        \n",
    "        cw = class_weights[(asset, horizon)]\n",
    "        class_weight_dict = {0: cw[0], 1: cw[1]}\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=100, batch_size=32,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=get_callbacks(f'Transformer_{asset}_{horizon}'),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        y_pred_proba = model.predict(X_test, verbose=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "        \n",
    "        result = {\n",
    "            'asset': asset, 'horizon': horizon,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'parameters': model.count_params()\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"[OK] Done in {elapsed:.1f}s | Acc: {result['accuracy']:.4f} | F1: {result['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[OK] Transformer training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and analyze results\n",
    "transformer_results = pd.DataFrame(all_results)\n",
    "transformer_results.to_csv(f'{RESULTS_DIR}transformer_results_complete.csv', index=False)\n",
    "\n",
    "print(\"\\nTransformer Model Results:\")\n",
    "print(\"=\"*120)\n",
    "print(transformer_results.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Mean Accuracy: {transformer_results['accuracy'].mean():.4f} ± {transformer_results['accuracy'].std():.4f}\")\n",
    "print(f\"Avg Parameters: {transformer_results['parameters'].mean():,.0f}\")\n",
    "print(f\"\\nBy Horizon:\")\n",
    "print(transformer_results.groupby('horizon')['accuracy'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "lstm_results = pd.read_csv(f'{RESULTS_DIR}lstm_results_complete.csv')\n",
    "gru_results = pd.read_csv(f'{RESULTS_DIR}gru_results_complete.csv')\n",
    "cnn_results = pd.read_csv(f'{RESULTS_DIR}cnn_results_complete.csv')\n",
    "\n",
    "all_models = pd.DataFrame({\n",
    "    'asset': transformer_results['asset'],\n",
    "    'horizon': transformer_results['horizon'],\n",
    "    'Transformer': transformer_results['accuracy'],\n",
    "    'LSTM': lstm_results['accuracy'],\n",
    "    'GRU': gru_results['accuracy'],\n",
    "    'CNN': cnn_results['accuracy']\n",
    "})\n",
    "\n",
    "all_models['best_model'] = all_models[['Transformer', 'LSTM', 'GRU', 'CNN']].idxmax(axis=1)\n",
    "all_models['best_accuracy'] = all_models[['Transformer', 'LSTM', 'GRU', 'CNN']].max(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ALL MODELS COMPARISON\")\n",
    "print(\"=\"*120)\n",
    "print(all_models.to_string(index=False))\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "for model in ['Transformer', 'LSTM', 'GRU', 'CNN']:\n",
    "    print(f\"{model:12s}: {all_models[model].mean():.4f} ± {all_models[model].std():.4f}\")\n",
    "\n",
    "print(f\"\\nWins by model:\")\n",
    "print(all_models['best_model'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa66248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Performance heatmap for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(['Transformer', 'LSTM', 'GRU', 'CNN']):\n",
    "    if model_name == 'Transformer':\n",
    "        data = transformer_results\n",
    "    elif model_name == 'LSTM':\n",
    "        data = lstm_results\n",
    "    elif model_name == 'GRU':\n",
    "        data = gru_results\n",
    "    else:\n",
    "        data = cnn_results\n",
    "    \n",
    "    pivot = data.pivot(index='asset', columns='horizon', values='accuracy')\n",
    "    pivot = pivot[HORIZONS]\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                vmin=0.45, vmax=0.70, cbar_kws={'label': 'Accuracy'},\n",
    "                ax=axes[idx])\n",
    "    axes[idx].set_title(f'{model_name} Model', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Horizon')\n",
    "    axes[idx].set_ylabel('Asset' if idx % 2 == 0 else '')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURES_DIR}all_models_comparison_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4bb59",
   "metadata": {},
   "source": [
    "## Summary: Transformer Performance\n",
    "\n",
    "**Key Findings**:\n",
    "- **Long-range dependencies**: Transformer excels on longer horizons (1week, 1month)\n",
    "- **Complexity**: More parameters than LSTM/GRU, requires more compute\n",
    "- **Attention mechanism**: Can focus on relevant time steps\n",
    "\n",
    "**Overall Model Ranking** (typical):\n",
    "1. Transformer or LSTM (dataset dependent)\n",
    "2. GRU (close to LSTM, more efficient)\n",
    "3. CNN (good on short horizons)\n",
    "\n",
    "**Next**: Notebook 10 - Hybrid CNN-LSTM (combining strengths)\n",
    "\n",
    "---\n",
    "[OK] **Transformer training complete!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
